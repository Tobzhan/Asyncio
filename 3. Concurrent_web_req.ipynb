{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrent web requests with asyncio and aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__aoihttp__ - library that uses non-blocking sockets to make web requests and returns coroutines for those requests, which we can then await for a result. <br>\n",
    "We have also __requests__ but it has only blocking sockets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering context manager, waiting for connection\n",
      "Accepted a connection\n",
      "b'\\x1b[A'\n",
      "Exiting context manager\n",
      "Closed connection\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import socket\n",
    "from types import TracebackType\n",
    "from typing import Optional, Type\n",
    "\n",
    "class ConnectedSocket:\n",
    "    def __init__(self, server_socket):\n",
    "        self._connection = None\n",
    "        self._server_socket = server_socket\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        print('Entering context manager, waiting for connection')\n",
    "        loop = asyncio.get_event_loop()\n",
    "        connection, address = await loop.sock_accept(self._server_socket)\n",
    "        self._connection = connection\n",
    "        print('Accepted a connection')\n",
    "        return self._connection\n",
    "    \n",
    "    async def __aexit__(self,\n",
    "                        exc_type: Optional[Type[BaseException]],\n",
    "                        exc_val: Optional[BaseException],\n",
    "                        exc_tb: Optional[TracebackType]):\n",
    "        print('Exiting context manager')\n",
    "        self._connection.close()\n",
    "        print('Closed connection')\n",
    "\n",
    "\n",
    "async def main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    server_socket = socket.socket()\n",
    "    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "    server_address = ('127.0.0.1', 8000)\n",
    "    server_socket.setblocking(False)\n",
    "    server_socket.bind(server_address)\n",
    "    server_socket.listen()\n",
    "\n",
    "    # We can use this method to catch requests and close them automatically \n",
    "    async with ConnectedSocket(server_socket) as connection:\n",
    "        data = await loop.sock_recv(connection, 1024)\n",
    "        print(data)\n",
    "    \n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sessions, where youâ€™ll keep many connections open, which can then be recycled. This is known as connection pooling. Connection pooling is an important concept that aids the performance of our aiohttp-based applications. <br>\n",
    "We will create one session for application. <br>\n",
    "A session object has methods on it for making any number of web requests, such as GET, PUT, and POST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting <function main at 0x000002337C132020> with args () {}'\n",
      "starting <function fetch_status at 0x000002337C132D40> with args (<aiohttp.client.ClientSession object at 0x000002337BFF6E50>, 'https://www.example.com') {}'\n",
      "finished <function fetch_status at 0x000002337C132D40> in 0.7393 second(s)\n",
      "Status for https://www.example.com was 200\n",
      "finished <function main at 0x000002337C132020> in 0.7403 second(s)\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from util.async_timed import async_timed\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "@async_timed()\n",
    "async def fetch_status(session : ClientSession, url: str) -> int:\n",
    "    async with session.get(url) as result:\n",
    "        return result.status\n",
    "\n",
    "@async_timed()\n",
    "async def main():\n",
    "    # We create session where we will work / we can have till 100 connections by default but we can change it\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        url = 'https://www.example.com'\n",
    "        status = await fetch_status(session, url)\n",
    "        print(f\"Status for {url} was {status}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set timeout for our requests by using __ClientTimeout__. By default it is 5 mins. We can change this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting <function main at 0x000002337C8FFA60> with args () {}'\n",
      "starting <function fetch_status at 0x000002337C8FF2E0> with args (<aiohttp.client.ClientSession object at 0x000002337C98C490>, 'https://www.example.com') {}'\n",
      "finished <function fetch_status at 0x000002337C8FF2E0> in 0.0000 second(s)\n",
      "finished <function main at 0x000002337C8FFA60> in 0.0010 second(s)\n",
      "Timeout error\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from util.async_timed import async_timed\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "@async_timed()\n",
    "async def fetch_status(session : ClientSession, url: str) -> int:\n",
    "    # Creating timeout for 100ms for fetching status\n",
    "    ten_millis = aiohttp.ClientTimeout(total=.01)\n",
    "\n",
    "    async with session.get(url, timeout = ten_millis) as result:\n",
    "        return result.status\n",
    "\n",
    "@async_timed()\n",
    "async def main():\n",
    "    # Creating timeout for 1 sec\n",
    "    session_timeout = aiohttp.ClientTimeout(total=1, connect=.1)\n",
    "\n",
    "    async with aiohttp.ClientSession(timeout = session_timeout) as session:\n",
    "        url = 'https://www.example.com'\n",
    "        status = await fetch_status(session, url)\n",
    "        print(f\"Status for {url} was {status}\")\n",
    "\n",
    "# We will get asyncio.TimeoutError if our reqests will exceed the time that we defined\n",
    "try:\n",
    "    await main()\n",
    "except asyncio.TimeoutError as f:\n",
    "    print(f\"Timeout error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will not work concurrently because we create and immedeately await it, so it will run as synchonous code\n",
    "\"\"\"\n",
    "async def main() -> None:\n",
    "    delay_times = [3, 3, 3]\n",
    "    [await asyncio.create_task(delay(seconds)) for seconds in delay_times]\n",
    "\"\"\"\n",
    "\n",
    "# To fix it firstly we need to create all tasks then await all of them like this\n",
    "\"\"\"\n",
    "async def main() -> None:\n",
    "    delay_times = [3, 3, 3]\n",
    "    tasks = [asyncio.create_task(delay(seconds)) for seconds in delay_times]\n",
    "    [await task for task in tasks]\n",
    " \"\"\"\n",
    "# Still this code is not perfect, if one task will get exception, then all tasks will be canceled but we can fix it with asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use __gather__ instead of these list comprehensions, also it can solve the problem above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting <function main at 0x0000016E4B299940> with args () {}'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Asyncio\\chapter_04\\__init__.py:6: RuntimeWarning: coroutine 'sleep' was never awaited\n",
      "  asyncio.sleep(delay)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "finished <function main at 0x0000016E4B299940> in 2.8252 second(s)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from aiohttp import ClientSession\n",
    "from chapter_04 import fetch_status\n",
    "from util.async_timed import async_timed\n",
    "\n",
    "# Lets 1000 times fetch status\n",
    "@async_timed()\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        urls = ['https://www.example.com' for _ in range(1000)]\n",
    "        requests = [fetch_status(session, url) for url in urls]\n",
    "        status_codes = await asyncio.gather(*requests)\n",
    "        print(status_codes)\n",
    "\n",
    "# Code is working concurrently\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very important: It is worth noting that the results for each awaitable we pass in may not complete in\n",
    "a deterministic order, A nice feature of gather is that, regardless of when\n",
    "our awaitables complete, we are guaranteed the results will be returned in the order\n",
    "we passed them in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with exceptions with gather: <br>\n",
    "* return_exceptions=False â€” This is the default value for gather. In this case, if any of our coroutines throws an exception, our gather call will also throw that exception when we await it. However, even though one of our coroutines failed, our other coroutines are not canceled and will continue to run as long as we handle the exception, or the exception does not result in the event loop stopping and canceling the tasks.\n",
    "* return_exceptions=True â€” In this case, gather will return any exceptions as part of the result list it returns when we await it. The call to gather will not throw any exceptions itself, and weâ€™ll be able handle all exceptions as we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting <function main at 0x000001C84D126480> with args () {}'\n",
      "[200, AssertionError()]\n",
      "finished <function main at 0x000001C84D126480> in 0.7804 second(s)\n"
     ]
    }
   ],
   "source": [
    "# What happen if we use wrong url:\n",
    "@async_timed()\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        urls = ['https://www.example.com', 'python://www.example.com']\n",
    "        requests = [fetch_status(session, url) for url in urls]\n",
    "        status_codes = await asyncio.gather(*requests, return_exceptions=1)\n",
    "        print(status_codes)\n",
    "\n",
    "# We get AssertionError frow wrong urls\n",
    "# Impoertant, even if we get exceptions, all tasks is running anyways\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages of __gather__:\n",
    "* It is hard with gather cancel all tasks if we get exception.\n",
    "* All tasks should end before we can do something with results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing requests as they complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting <function main at 0x0000016E4E354720> with args () {}'\n",
      "200\n",
      "200\n",
      "200\n",
      "finished <function main at 0x0000016E4E354720> in 10.1944 second(s)\n"
     ]
    }
   ],
   "source": [
    "async def fetch_status(session : ClientSession, url: str, delay: int = 0) -> int:\n",
    "    await asyncio.sleep(delay)\n",
    "    async with session.get(url) as result:\n",
    "        return result.status\n",
    "    \n",
    "@async_timed()\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        fetchers = [fetch_status(session, 'https://www.example.com', 1),\n",
    "                    fetch_status(session, 'https://www.example.com', 1),\n",
    "                    fetch_status(session, 'https://www.example.com', 10)]\n",
    "        \n",
    "        # Gather is waiting all 10 seconds  \n",
    "        \"results = await asyncio.gather(*fetchers)\"\n",
    "        # finished <function main at 0x0000016E4B77E7A0> in 10.1961 second(s)\n",
    "\n",
    "        # as_completed returns tasks the moment when they finish not waiting other tasks\n",
    "        for finished_tasks in asyncio.as_completed(fetchers):\n",
    "            print(await finished_tasks)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@async_timed()\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        fetchers = [fetch_status(session, 'https://www.example.com', 10),\n",
    "                    fetch_status(session, 'https://www.example.com', 10),\n",
    "                    fetch_status(session, 'https://www.example.com', 1)]\n",
    "\n",
    "        # as_completed returns tasks the moment when they finish not waiting other tasks\n",
    "        for finished_tasks in asyncio.as_completed(fetchers, timeout=2):\n",
    "            try:\n",
    "                print(await finished_tasks)\n",
    "            except asyncio.TimeoutError as f:\n",
    "                print(f\"TimeoutError\")\n",
    "\n",
    "# The order os tasks is unpredictable\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Important__: Even if we get exception, tasks will continue to run as for both gather and as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting <function main at 0x0000016E4B77EB60> with args () {}'\n",
      "Done task count: 4\n",
      "Pending task count: 0\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "finished <function main at 0x0000016E4B77EB60> in 1.7736 second(s)\n"
     ]
    }
   ],
   "source": [
    "# We can kinda solve problem of running tasks even after getting exception with wait\n",
    "@async_timed()\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        fetchers = [asyncio.create_task(fetch_status(session, 'https://www.example.com', 1) ) for _ in range(4) ]\n",
    "\n",
    "        done, pending = await asyncio.wait(fetchers)\n",
    "\n",
    "        print(f\"Done task count: {len(done)}\")\n",
    "        print(f\"Pending task count: {len(pending)}\")\n",
    "\n",
    "        for done_task in done:\n",
    "            result = await done_task\n",
    "            print(result)\n",
    "    \n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting <function main at 0x0000016E4E356A20> with args () {}'\n",
      "Done task count: 2\n",
      "Pending task count: 0\n",
      "200\n",
      "Error\n",
      "finished <function main at 0x0000016E4E356A20> in 1.7361 second(s)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# We can kinda solve problem of running tasks even after getting exception with wait\n",
    "@async_timed()\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        fetchers = [asyncio.create_task(fetch_status(session, 'https://www.example.com', 1) ),\n",
    "                    asyncio.create_task(fetch_status(session, 'python://www.example.com', 1) )]\n",
    "\n",
    "        done, pending = await asyncio.wait(fetchers)\n",
    "\n",
    "        print(f\"Done task count: {len(done)}\")\n",
    "        print(f\"Pending task count: {len(pending)}\")\n",
    "\n",
    "        for done_task in done:\n",
    "            # result = await done_task -> we can get exception\n",
    "            if done_task.exception() is None:\n",
    "                print(done_task.result())\n",
    "            else:\n",
    "                # logging.error(\"Error\", exc_info=done_task.exception())\n",
    "                print(\"Error\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL_COMLETED works like gather, so it means if we get exceptions, we will not see the until all tasks ends <br>\n",
    "FIRST_EXCEPTION it will return done, pending immediately if it will face exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting <function main at 0x0000016E4E3565C0> with args () {}'\n",
      "Done task count: 1\n",
      "Pending task count: 2\n",
      "Error\n",
      "finished <function main at 0x0000016E4E3565C0> in 1.0173 second(s)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "@async_timed()\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        fetchers = [asyncio.create_task(fetch_status(session, 'python://www.example.com', 1) ),\n",
    "                    asyncio.create_task(fetch_status(session, 'https://www.example.com', 3) ),\n",
    "                    asyncio.create_task(fetch_status(session, 'https://www.example.com', 5) )]\n",
    "\n",
    "        done, pending = await asyncio.wait(fetchers, return_when=asyncio.FIRST_EXCEPTION)\n",
    "\n",
    "        print(f\"Done task count: {len(done)}\")\n",
    "        print(f\"Pending task count: {len(pending)}\")\n",
    "\n",
    "        for done_task in done:\n",
    "            # result = await done_task -> we can get exception\n",
    "            if done_task.exception() is None:\n",
    "                print(done_task.result())\n",
    "            else:\n",
    "                # logging.error(\"Error\", exc_info=done_task.exception())\n",
    "                print(\"Error\")\n",
    "\n",
    "        for pending_task in pending:\n",
    "            pending_task.cancel()\n",
    "\n",
    "# Here we get exception and cancel all pending tasks\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change behaviour of __wait__ to work as __as_completed__ using FIRST_COMPLETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting <function main at 0x0000016E4E355580> with args () {}'\n",
      "Done task count: 1\n",
      "Pending task count: 2\n",
      "200\n",
      "Done task count: 1\n",
      "Pending task count: 1\n",
      "200\n",
      "Done task count: 1\n",
      "Pending task count: 0\n",
      "200\n",
      "finished <function main at 0x0000016E4E355580> in 5.1975 second(s)\n"
     ]
    }
   ],
   "source": [
    "@async_timed()\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        fetchers = [asyncio.create_task(fetch_status(session, 'https://www.example.com', 1) ),\n",
    "                    asyncio.create_task(fetch_status(session, 'https://www.example.com', 3) ),\n",
    "                    asyncio.create_task(fetch_status(session, 'https://www.example.com', 5) )]\n",
    "\n",
    "        pending = fetchers\n",
    "\n",
    "        while pending:\n",
    "            done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "\n",
    "            print(f\"Done task count: {len(done)}\")\n",
    "            print(f\"Pending task count: {len(pending)}\")\n",
    "\n",
    "            for done_task in done:\n",
    "                # result = await done_task -> we can get exception\n",
    "                if done_task.exception() is None:\n",
    "                    print(done_task.result())\n",
    "                else:\n",
    "                    # logging.error(\"Error\", exc_info=done_task.exception())\n",
    "                    print(\"Error\")\n",
    "\n",
    "# Now it works like as_comleted but now we can actually have data about done tasks and pednig tasks \n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting <function main at 0x0000016E4E3565C0> with args () {}'\n",
      "Done task count: 1\n",
      "Pending task count: 2\n",
      "200\n",
      "finished <function main at 0x0000016E4E3565C0> in 3.0033 second(s)\n"
     ]
    }
   ],
   "source": [
    "@async_timed()\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        fetchers = [asyncio.create_task(fetch_status(session, 'https://www.example.com', 1) ),\n",
    "                    asyncio.create_task(fetch_status(session, 'https://www.example.com', 3) ),\n",
    "                    asyncio.create_task(fetch_status(session, 'https://www.example.com', 5) )]\n",
    "\n",
    "        done, pending = await asyncio.wait(fetchers, return_when=asyncio.ALL_COMPLETED, timeout=3)\n",
    "\n",
    "        print(f\"Done task count: {len(done)}\")\n",
    "        print(f\"Pending task count: {len(pending)}\")\n",
    "\n",
    "        for done_task in done:\n",
    "            # result = await done_task -> we can get exception\n",
    "            if done_task.exception() is None:\n",
    "                print(done_task.result())\n",
    "            else:\n",
    "                # logging.error(\"Error\", exc_info=done_task.exception())\n",
    "                print(\"Error\")\n",
    "\n",
    "        for pending_task in pending:\n",
    "            pending_task.cancel()\n",
    "\n",
    "# apllying timeout after which we should cancel all pending tasks for that moment\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our own asynchronous context managers: __async with__\n",
    "* __aiohttp__ library to make asynchronous web requests\n",
    "* __gather__, __as_completed__, __wait__, working with exceptions, timeouts and cancelling tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
